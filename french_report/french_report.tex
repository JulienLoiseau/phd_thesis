\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{python}
\usepackage{algpseudocode,algorithm}
\usepackage{makeidx}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{multicol}

\usepackage{tikz}
\usetikzlibrary{matrix}
\usetikzlibrary{positioning}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{patterns}

\renewcommand{\thesection}{\arabic{section}}


\newcommand\todo[1]{\textcolor{red}{#1}}
\newcommand{\locpath}{.}


\usepackage[left=1.5cm,right=1.5cm,top=2cm,bottom=2.5cm]{geometry}

% Hyphenation
\hyphenation{bench-mark}
\hyphenation{li-bra-ry}
\hyphenation{re-porti}
\hyphenation{Tech-ni-cal}
\hyphenation{Avail-able}
\hyphenation{PRO-GRAM-MING}

\title{TH\`ESE de Doctorat: Résumé français\\Les Architectures Hybrides pour Atteindre l'Exascale}
\author{Julien LOISEAU}
\date{1 juin 2018}

\begin{document}


%% ------------------------------------------------------------------
%% DATA OF TITLE PAGE 
%% ------------------------------------------------------------------
\newcommand{\phdAuthor}{Julien LOISEAU}
\newcommand{\defenseDate}{le 18 Mars 2018}
\newcommand{\phdDiscipline}{Informatique}
\newcommand{\phdSpeciality}{Calcul Haute Performance}
\newcommand{\phdTitleEN}{Hybrids Architectures to Reach Exascale}
\newcommand{\phdTitleFR}{Les Architectures Hybrides pour Atteindre l'Exascale}
\newcommand{\phdDirector}{Michaël KRAJECKI, Professeur des Universités}

%% ------------------------------------------------------------------

\newgeometry{left=0.5cm,right=0.5cm,top=1.0cm,bottom=1.5cm}
%% Title page -------------------------------------------------------
\thispagestyle{empty}
{
\centering

% LOGO URCA
{
	\includegraphics[scale=1]{../figures/style/logo_urca_front_page.jpg}
	\vspace{0.5cm}
}

% UNIVERSITE ET ECOLE DOCTORALE
{
	UNIVERSIT\'E DE REIMS CHAMPAGNE-ARDENNE\\
	\vspace{0.5cm}
	\'Ecole Doctorale Sciences Technologie Sant\'e\\
	\vspace{1cm}
	\huge
	\textbf{Rapport français}
	\vspace{.5cm}
}

%% TITRE ET GRADE + DISCIPLINE ET SPECIALITY
{
	Pour obtenir le grade de:\\ 
	\vspace{.4cm}
	\Large 
	\textbf{Docteur de l'Université de Reims Champagne-Ardenne}\\
	\vspace{.4cm}
	\large
	\textbf{\textit{Discipline : }\phdDiscipline}\\
	\vspace{.4cm}
	\textbf{\textit{Sp\'ecialit\'e : }\phdSpeciality}
}

%% AUTHOR
{
	\vspace{1cm}
	Pr\'esent\'ee et soutenue par:\\
	\vspace{.4cm}
	\Large
	\textbf{\phdAuthor}\\
	\vspace{.4cm}
	\large
	\defenseDate
	\vspace{.5cm}	
}

{
	%% LIGNE SEPARATION 
	\hrule height 2pt
	\vspace{2pt}
	\huge 
	% TITRE EN 
	\vspace{1cm}
	\phdTitleFR\\
	\large
	% TITRE FR 
	%\vspace{0.5cm}
	%\phdTitleEN
	%% LIGNE SEPARATION
	\vspace{1.0cm}
	\hrule
	\vspace{1cm}
}

%% UNDER DIRECTION OF 
{
	Sous la direction de :\\ 
	\textbf{\phdDirector}
	\vspace{0.5cm}
	\vspace{\fill}
}

%% JURY 
{
\normalsize
\begin{tabular}{l l l}
		\textbf{JURY} &  & \\
		&&\\
		Pr. Michaël Krajecki & CReSTIC, Université de Reims Champagne-Ardenne & Directeur \\
		Dr. François Alin & CReSTIC, Université de Reims Champagne-Ardenne & Co-directeur  \\
		Dr. Christophe Jaillet & CReSTIC, Université de Reims Champagne-Ardenne & Co-directeur \\
		Pr. William Jalby & Université de Versailles Saint-Quentin & Rapporteur\\
		Dr. Benjamin Bergen & Ingénieur au Los Alamos National Laboratory, USA & Rapporteur \\
		Pr. Zineb Habbas & LCOMS, Université de Lorraine & Rapporteur \\ 
		Pr. Françoise Baude & I3S, Université de Nice Sophia Antipolis& Examinateur\\
		Guillaume Colin de Verdiere & Ingénieur au CEA & Invit\'e
\end{tabular}
\vspace{0.5cm}
\hrule
\vspace{0.3cm}
}

%% LABO 
{
	Centre de Recherche en Sciences et Technologies de l'Information et de la Communication de l'URCA EA3804
}
% EQUIPE
{
	\'Equipe Calcul et Autonomie dans les Systèmes Hétérogènes
}

%%%
}

\clearpage 
\newgeometry{left=2.5cm,right=2.5cm,top=2.0cm,bottom=2.5cm}


Ce document comprend un résumé substantiel de ma thèse rédigée en anglais.
Il présente tout d'abord la traduction de la table des matières de la thèse. 
Une traduction de l'introduction générale, une synthèse des trois parties de la thèse ainsi que de la conclusion générale. 

\section{Table des matières traduite en français}
Cette traduction a été réduite aux parties, chapitres et sections de la thèse. 

% Add the tableofcontentes in itself
%\newgeometry{left=1cm,right=.3cm,top=2.0cm,bottom=2.5cm}
\vspace{.2cm}
\hrule
\vspace{.2cm}
\makeatletter
\input{jloiseau_these.toc}
\makeatother
\vspace{.2cm}
\hrule
\newpage

\section{Introduction}

Le monde du calcul haute performance (HPC) va prochainement atteindre une puissance de calcul inégalée avec l'Exascale.
Les \'Etats Unis D'Amériques et l'Europe devraient l'atteindre aux horizons 2020-2021 mais la Chine pourrait proposer une telle machine dès 2019.
Ces superordinateurs seront $100$ fois plus rapides que l'estimation actuelle des performances du cerveau humain avec $10^{16}$ calculs à virgules flottante par secondes (FLOPS)\cite[kurzweil2010singularity] et atteindre une puissance sans précédent d'un milliard de milliard ($10^{18}$) de FLOPS.
Cette aventure a commencée avec les premiers ordinateurs à tube à vides et les besoins de la balistique pour la guerre. 
De nos jours les supercalculateurs étendent leur domaime d'application et sont un élément phare pour représenter la puissance d'une nation.
Les applications se sont répandus dans tous les secteurs de la science et de la technologie.\\

Depuis 1962, et en considérant le Cray CDC 6600 tel que le premier superordinateur, la puissance de calcul des ces machines n'a fait qu'augmenter tout en suivant une observation faite par le co-fondateur de l'entreprise Intel, Gordon Moore. 
Mieux connue sous la nom de "Loi de Moore", cette loi de 1965 explique que, considérant l'évolution constante de la technologie, le nombre de transistors sur un circuit intégré va doubler environs tous les deux ans pour un prix similaire. 
Ceci va permettre une augmentation parallèle de la puissance de calcul que peux fournir les ordinateurs et superordinateurs. 
Plus important encore, comme "\textit{L'argent est le nerf de la guerre}", le prix des puces pour des performances meilleures va diminuer. 

Cette observation de Gordon Moore peut-être observée avec l'évolution de la puissance des supercalculateur, la liste TOP500\footnote{https://www.top500.org}.
Présenté sur la figure~\ref{fig:intro_top500}, le loi de Moore se poursuit et reste vraie malgré des décénnies.

\begin{figure}[h]
\centering
\includegraphics[width=.7\linewidth]{../figures/introduction/top500_list_approximation.png}
\caption{\'Evolution de la puissance de calcul, liste du TOP500}
\label{fig:intro_top500}
\end{figure}

La diminution de la taille des semi-conducteurs avec des transistors de plus en plus petits n'est pas la seule raison de cette évolution linéraire. 
Le premier processeurs étaient batis autour d'un unique coeur de calcul (CPU) avec une augmentation du nombre de transistors et une meilleure fréquence d'opération.
Ils ont vite rencontré une limitation pour atteindre des fréquences toujours plus élevées du fait de l'énergie requise mais aussi de la chaleur générée devant être canalisée.
C'est pourquoi, au début du vingtième siècle, IBM proposa le premier processeur multi-core, le Power4.
Les constructeurs ont commencés à créer des puces avec plus de un coeur pour augmenter la puissance de calcul tout en continuant à réduire la taille des composants. 
Cela permis de répondre à la constante demande en puissance de calcul et la Loi de Moore fut conservée. 
Bien entendu cette nouvelle architecture avait plusieurs problèmes. 
Un coût supplémentaire en synchronisation entre les coeurs pour l'accès à la mémoire, le partage des tâches mais aussi une complexité des algorithmes parallèles. 
Des nos jours un CPU classique propose de deux à des dizaines de coeurs de calcul sur une seule carte.\\



In order to reach even more computational power some researchers started to use many-core approaches. 
By using hundreds of cores, these devices take advantage of very "simple" computing units, with slow frequency and low power consumption but add more complexity and requirement for their efficient programming with even more synchronizations needed between the cores. 
Typically, those many-core architectures are used coupled with a CPU that sends the data and drives them.
Some accelerators like the Intel Xeon Phi can be driven or driver depending on their configuration. 
Usually called accelerators, those devices are used in addition to the host processor to provide their efficient computational power in the key part of execution. 
The most famous accelerators are the Xeon Phi, the General-Purpose Graphics Processing Unit (GPGPU), initially used for graphic processing, Field Programmable Gates Array (FPGA) or dedicated Application-Specific Integrated Circuit (ASIC).
The model using a host with additional device(s) appears and we will be referred to as "Hybrid Architecture".
In fact, a cluster can be composed of CPUs, CPUs with accelerator(s) of the same kind, CPUs with heterogeneous accelerators or even accelerators like Xeon Phi driving different kinds of accelerators.\\

Since either 2013 or 2014 many companies, like the Gordon Moore's company \textit{Intel} itself, stated that the Moore's law is over. 
This can be seen on figure~\ref{fig:intro_top500}: on the right part of the graph, the evolution is no longer linear and tends to decrease slowly in time. 
This can be contributed to two main factors. 
First, we slowly reach the maximal shrink size of the transistors implying hard to handle side effects. 
Second, the power wall implied by the power consumption required by so many transistors and frequency speed on the chip.

Even with all these devices, current supercomputers face several problems in their conception and utilization. 
The three main walls bounding the overall computational power of the machines are: the power consumption wall, the communication wall and the memory wall.  
Sub-problems like the interconnect wall, resilience wall or even the complexity wall also arise and make the task even more difficult.\\

In this context of doubts and questions about the future of HPC, this study proposes several points of view. 
We believe the future of HPC is made with these hybrid architectures or acceleration devices adapted to the need, using well suited API, framework and code.
We consider that the classical benchmarks, like the TOP500, are not enough to target the main walls of those architectures, especially accelerators. 
Domain scientists’ applications like physics/astrophysics/chemist/biologist require benchmarks based on more irregular cases with heavy computation, communications and memory accesses. 

In this document, we propose a metric that extracts the three main issues of HPC and apply them to accelerated architectures to determine how to take advantage of these architectures and what are the limitations for them. 
The first step of this metrics is obtained when merging two characteristic problems and then a third problem, combining all our knowledge.
The first two are targeting computation and communication wall over very irregular cases with high memory accesses, using an academic combinatorial problem and the Graph500 benchmark. 
The last is a computational scientific problem that will cover both difficulties of the previous problems and appears to be hard to implement on supercomputers and even more on accelerated ones.
The results obtain supports our thesis and show hybrid architectures as the best solution to reach Exascale.\\

Cette thèse se décompose en trois parties.

The first part explores the state of the art in HPC from the main laws to the hardware. 	
We go through the basic laws from Amdahl's to Gustafson's laws and the specification of speedup and efficiency.
Classical CPUs, GPGPUs and other accelerators are described and discussed regarding the state of the art. 
The main methods of ranking and the issues regarding them are presented.\\ 

In the second part we propose our metric based on characteristic problems to target classical and hybrid architectures.
The Langford problem is described as an irregular and computationally heavy problem.
This demonstrates how the accelerators, in this case GPUs, are able to support the memory and computation wall. 
This work leads to the publication of one journal paper \cite{krajecki2016many} and many conferences, presentations and posters \cite{deleau2014towards,j2016resolution,jaillet2014Langford}.
Our implementation of the Langford problem allowed us to beat a world record with the last instances of this academic problem.

The Graph500 problem is then proposed as an irregular and communications heavy problem. 
We present our implementation, and moreover, the logic to take advantage of the GPUs computational power for characteristic applications. 
This work leaded to the publication of a conference paper \cite{krajecki2016bfs} and many presentations and posters \cite{loiseau2015parcours,loiseau2015GTC}.\\

In the third part, we consider a problem that is substantial and irregular in regards to computation and communications.
We analyze this problem and show that it combines all the previous limitations. 
Then we apply our methodology and show how modern supercomputers can overcome these issues. 
This computational science problem is based on the Smoothed Particle Hydrodynamics method.
The former application began with the development of the FleCSI framework from the Los Alamos National Laboratory which allowed us to exchange directly with the LANL domain scientists on their needs.
We intent to provide an efficient tool for physicists and astrophysicists, called FleCSPH, based on our global work to efficiently parallelize these types of production applications
This work leaded to several presentation and posters \cite{debrye20162HOT,loiseau2017SC}.\\

The last part will summarize on this work and results to show some of the main prospects of this study and my future researches.

\section{Modèles du HPC}

\section{Métrique de problème complexes}

\section{Application}

\section{Conclusion}


\section*{Bibliographie}
\bibliographystyle{alpha}
\bibliography{../biblio/my_papers.bib,../biblio/biblio_langford,../biblio/biblio_graph,../biblio/biblio_sph,../biblio/biblio_hpc,../biblio/biblio_part2_chap1}



\end{document}
