%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																	%
%	CHAPTER ONE, THEORY of HPC										%
%																	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Theory of HPC}

\section{Introduction}

High Performance Computing (HPC) takes his roots from the beginning of computer odyssey in the middle 20th century.
A lot of rules, observations, theories and even Computer Science field itself emerged from it. 
In order to understand and characterize HPC and supercomputers, some knowledge on theory is required. 
This part describes the Von Neumann model, the generic model of computer on which every nowadays machine is built.
It is presented along with the Flynn taxonomy that is a classification of the different execution models. 
Base on those elements we also present the differents shared memory models. 

We then give more details on what is and how to reach performances though parallelism. 
And thus we need to define what performance implies in HPC. 

The Amhdal's and Gustafson's laws are presented and detailed and thus the Strong and Weak scaling used in our study. 

\section{Von Neumann Model}
\index{Von Neumann Model}
First computers, in early 20th, were built using vacuum tubes making them high power consuming, hard to maintain and expansive to create.
The most famous of first vacuum tubes supercomputers, The ENIAC, was based on decimal system.
It might be the most known of first supercomputers but the real revolution came from its successor.
In 1944 the first binary system based computer, called the Electric Discrete Variable Automatic Computer (EDVAC), was created. 
In the EDVAC team, a physists described the logical model of this computer and provides a model on which every nowadays computing device is based. 

\begin{figure}
\centering 
\begin{tikzpicture}
\draw (3,0) rectangle (8,4);
\draw (3.2,1.3) [fill=red!30] rectangle (7.8,3.8);
\node at (5.5,3.5) {Central Processing Unit};

\draw (3.3,1.5) [fill=white] rectangle (7.7,2.) node[pos=.5] {Registers};
\draw (3.3,2.1) [fill=white] rectangle (7.7,2.6) node[pos=.5] {Arithmetic/Logic Unit};
\draw (3.3,2.7) [fill=white] rectangle (7.7,3.2) node[pos=.5] {Control Unit};

\draw (0,1.5) rectangle (2.5,2.5) node[pos=.5] {Input device};
\draw (8.5,1.5) rectangle (11,2.5) node[pos=.5] {Output device};
\draw [->] (2.5,2) -- (3,2);
\draw [->] (8,2) -- (8.5,2);

\draw (3.2,.2) [fill=blue!20] rectangle (7.8,.8) node[pos=.5] {Memory};
\draw [->] (4,1.3) -- (4,.8);
\draw [->] (7,.8) -- (7,1.3);
\end{tikzpicture}
\caption{Von Neumann model}
\label{fig:1_HPC:von_neumann_model}
\end{figure}

John Von Neumann published its \textit{First Draft of a Report on the EDVAC}~\cite{von1993first} in 1945. 
Extracted from this work, the model know as the Von Neumann model or more generally Von Neumann Machine appears. 
The model is presented on Fig.~\ref{fig:1_HPC:von_neumann_model}.

On that figure we can identify three parts, the input and output devices and in the middle the computational device itself. 
\paragraph{Input/Output devices}
The input and output devices are used to store in a read/write way data. 
They can be represented as hard drives, solid state drives or even monitors or printers. 

Inside the computational device we find the memory, for the most common nowadays architectures it can be considered as a Random Access Memory (RAM). 
Several kind of memory exists and will be discussed later. 
\paragraph{Central Processing Unit}
The Central Processing Unit, CPU, is composed of several elements in this model. 
On one hand, the Arithmetic and Logic Unit, ALU, which takes as input one or two values and apply an operation on those data. 
They can be either logics with operations such as AND, OR, XOR, etc. or arithmetique with operations such as ADD, MUL, SUB, etc. 
Of course those operations are way more complex on modern CPUs. 
On the other hand, the Control Unit, CU, which control the data carriage to the ALU from the memory and the operation to be perform on data.
It is also the part that takes care of the Program Counter (PC), the address of the next instruction in the program. 
We can also identify the Register section which represent data location used for both ALU and CU to store temporary results, the current instruction address, etc. 
Some representation may vary, the Registers can be represented directly inside the ALU or the CU. 
\paragraph{Buses}
The links between those elements are important and called Buses and can be separated between data buses, control buses and adresses buses.\\

The usual processing flow on such an architecture can be summarized as a loop: 
\begin{itemize}[noitemsep,nolistsep]
\item[-] Fetch next instruction from memory;
\item[-] Decode instruction using the Instruction Set Architecture (ISA). Known ISA are Reduce Instruction Set Architecture (RISC) and Complex Instruction Set Architecure (CISC);
\item[-] Evaluate operand(s) address(es);
\item[-] Fetch operand(s) from memory;
\item[-] Execute operation(s), with some instructions sets and new architectures several instructions can be processed in the same clock time;
\item[-] Store results, increase PC; 
\end{itemize}

Every devices or machines we will describe in the next chapter will have the same architecture as a basis. 

\subsection{Terminology}
Before characterizing the execution models, some terminology must be set to describe properly the machines. 

\begin{description}[noitemsep,nolistsep]
\item[Core:] A core is a Von Neumann machine. 
\item[Socket/Host:] A socket is mistakenly called a CPU in nowadays language. It is, for multi-cores sockets, composed of several cores. The name Host comes from the Host-Device architecture using accelerators. 
\item[Accelerators/Devices:] Accelerators are devices that, in addition to the CPU, provide additional computation power. We can identify them as GPUs, FPGAs, ASICs, etc. A socket can have access to one or more accelerators and sockets can also share their usage. 
\item[Node:] A node regroup one or more sockets that usually share memory and, linked to the sockets, one or more accelerators. 
\item[Cluster/Supercomputer] The cluster group several nodes though an interconnect network.
	% Homogeneous/Heterogeneous  
\end{description}

\section{Flynn taxonomy and executions models}
\index{Flynn taxonomy}
The Von Neumann model gives us a generic idea of how a computational unit is fashioned. 
The constant demand in more powerful computers required the scientists to find more way to provide this computational capacity.
In 2001, IBM proposed the first multi-core processor on the same die, the Power4 with its 2 cores.
This evolution required new paradigms.
A right characterization is then essential to be able to target the right architecture for the right purpose. 
The flynn taxonomy presents a hierarchical organization of computation machines and executions models.

\begin{table}
\begin{center}
\[\arraycolsep=1.4pt\def\arraystretch{2.2}
\begin{tabular}{| l | l | l |}
\hline
Instruction(s) stream(s) / Data stream(s)	& Single Data (SD) 	& Multiple Data (MD) \\
\hline
Single Instruction (SI)		& SISD			& SIMD \\
\hline
Multiple Instructions (MI) 	& MISD		& MIMD \\
\hline
\end{tabular}
\]
\end{center}
\caption{Flynn's taxonomy}
\label{fig:1_HPC:flynn_taxonomy}
\end{table}

In this classification~\cite{flynn1972some} from 1972, Michael J. Flynn presents the SIMD, SISD, MISD and MIMD models represented on Fig.~\ref{fig:1_HPC:flynn_taxonomy}.
Every of those for execution model correspond to a specific machine and function.

\subsection{Single Instruction, Single Data: SISD}
This is the model corresponding to a single core CPU like in the Von Neumann model. 
This sequential model takes one instruction, operates on one data and the result is then store and the process continues over. 
SISD is important to consider as a reference for computational time and will be considered in the next part for Amdahl's and Gustafson's laws.

\subsection{Single Instruction, Multiple Data: SIMD}
This is the execution model corresponding to a many-core architecture like a GPU. 
SIMD can be extended from 2 to 16 elements for classical CPUs to hundreds and even thousands of core for GPGPUs. 
In the same clock, the same operation is executed on every process on different data. 
The best example stay the work on matrices like a stencil, same instruction executed on every element of the matrix. 

\subsection{Multiple Instructions, Multiple Data: MIMD}
Every element executes its own instructions on its own data set. 
This can represent the behavior of a CPU using several cores, threads or even the differents nodes of a supercomputer cluster. 

\subsection{Multiple Instructions, Single Data: MISD}
This last model can correspond to a pipelined computer but even in this case the data are modified after every operations.
This is the least common exection model.

\subsection{SIMT}
\index{SIMT}
We can also find another characterization to describe the new GPUs architecture: Single Instruction, Multiple Threads. 
This appears in one of NVIDIA's company paper~\cite{lindholm2008nvidia}. 
This model describes a stack of SIMD architectures, every block of threads is working with the same control processor on different data. 
This is the model we describe in next chapter used for the \textit{warps} model in NVIDIA CUDA.

\section{Memory}
In addition of the execution model and parallelism the memory accesses parterns have a main role on performances especially in SIMD and MIMD. 

Different memory technologies exists and the aim is always greater capacity, better speed and bandwidth while keeping the data integrity.

\subsection{Memory technologies}
We present here the volatile memory, represented in the memory part of the Von Neumann model. 
%\todo{Hard drives ??}

\subsubsection{SRAM}
The Static Random Access Memory is built using so called "flip-flop" circuits that can store the data as long as the machine is powered. 
This kind of memory is very expensive to produice due to the number of component needed and the size of the memory.
Therefore it is usually limited for small amount of storage. 
The SRAM is mainly used for cache memory. 

Cache is a memory mecanism that is useful to consider when targeting performance. 
This little memory is built over several levels. 
The closer to the CPU is L1, then L2 and generally no more than L3 except on specific architecture. 
When look for a data the CP will first check the L1 cache, otherwise L2 and L3 to get the data to higher level. 
This is based on the idea that if a data is used, it shall be use again in the near future.
Many cache architectures exist like direct, associative, fully associative, etc but this is behond the scope of this document. 

\subsubsection{DRAM}
\index{ECC}
The Dynamic Random Access Memory, at the opposite to the SRAM, is based on transistors and capacitors to store the binary information.
This memory is less expansive to produice but needs to be refresh at a determined time however the data are lost. 
There is several sub categories of DRAM used in different devices. 

Depending on the way the bus are used we can find Single Data Rate, SDR, Double Data Rate, DDR and QDR, Quad Data Rates DRAM memories. 
The number of data carried can go from 1x to 4x but the limitation of those products is the price of memory constantly rising. 

We can also find Error-Correcting Code, ECC, memory which implements a bunch of data correction algorithm be garanty the validity of them when error is not allowed. 

\todo{MCDRAM}
\todo{3D memory}

\begin{figure}
\centering 
\begin{tikzpicture}[
every node/.style = {
level distance=1em,
shape=rectangle, 
rounded corners,
draw, 
align=center,
    top color=white%, 
   % bottom color=blue!20
   }]]
   \node {MIMD} [sibling distance=12em]
   child { node {Shared} [sibling distance=7em]
   child{node {UMA}} 
   child{node {NUMA}
   child{node {CC-NUMA}}
   child{node {NC-NUMA}}
   }
   child{node {COMA}}
   }
   child { node {Distributed}
   child { node {NoRMA}}
   };
   \end{tikzpicture}
   \caption{MIMD memory models}
   \label{fig:1_HPC:mimd_memory_model}
   \end{figure}

   The different types of memory for MIMD model are summed up in Fig.\ref{fig:1_HPC:mimd_memory_model}.
   Two main categories can be extract, share or distributed memories. 

   \subsection{Shared memory}
   In case of the SISD the memory access is just serial and no really rules needs to be set for its usage. 
   When it comes to multi-threaded and multi-cores like MIMD or SIMD execution models several kind of memory models are possible. 
   We give a description of the most common shared memories architecures. 

   \hspace*{-2cm}
   \begin{figure}
   \centering 
   \begin{tikzpicture}
   \draw [rounded corners=15pt,fill=red!40] (0,0) rectangle (2,2) node[pos=.5] {CPU};
   \draw [rounded corners=15pt,fill=red!40] (3,0) rectangle (5,2) node[pos=.5] {CPU};

   \draw (0,2.5) rectangle (5,3.5) node[pos=.5] {Memory Controller};

   \draw (0,4) [fill=blue!20] rectangle (2,4.6) node[pos=.5] {RAM};
   \draw (0,5.2) [fill=blue!20] rectangle (2,5.8) node[pos=.5] {RAM};

   \draw (3,4) [fill=blue!20] rectangle (5,4.6) node[pos=.5] {RAM};
   \draw (3,5.2) [fill=blue!20] rectangle (5,5.8) node[pos=.5] {RAM};

   \draw [-] (2,1) -- (3,1);
   \draw [-] (2.5,1) -- (2.5,2.5);
	%\node at (2.5,2.2) {SDR, DDR, QDR};
	\node at (2.5,1.8) {links};

	\draw [-] (2.5,3.5) -- (2.5,5.5);
	\draw [-] (2,4.3) -- (3,4.3);
	\draw [-] (2,5.5) -- (3,5.5);
	\end{tikzpicture}
	\hspace{1cm}
	\begin{tikzpicture}
	\draw [rounded corners=15pt,fill=red!40] (0,0) rectangle (2,2) node[pos=.5] {CPU};
	\draw (3,0.1) [fill=blue!20] rectangle (5,0.7) node[pos=.5] {RAM};
	\draw (3,1.3) [fill=blue!20] rectangle (5,1.9) node[pos=.5] {RAM};
	\draw [-] (2,0.4) -- (3,0.4);
	\draw [-] (2,1.6) -- (3,1.6);

	\draw [rounded corners=15pt,fill=red!40] (3,3.8) rectangle (5,5.8) node[pos=.5] {CPU};
	\draw (0,4) [fill=blue!20] rectangle (2,4.6) node[pos=.5] {RAM};
	\draw (0,5.2) [fill=blue!20] rectangle (2,5.8) node[pos=.5] {RAM};
	\draw [-] (2,4.2) -- (3,4.2);
	\draw [-] (2,5.4) -- (3,5.4);

	\draw [<->] (1,2) -- (4,3.8);
	\node at (1.5,3) {QPI-LDT};
	\end{tikzpicture}
%\hspace{1cm}
%\begin{tikzpicture}
%	\draw [rounded corners=15pt] (1,0) rectangle (3,2) node[pos=.5] {CPU};
%	\draw [rounded corners=15pt] (1,3.8) rectangle (3,5.8) node[pos=.5] {CPU};
%
%	\draw (0,2.1) rectangle (2,2.8) node[pos=.5] {RAM};
%	\draw (0,3.2) rectangle (2,3.8) node[pos=.5] {RAM};
%	
%	\draw (3,2.2) rectangle (5,2.8) node[pos=.5] {RAM};
%	\draw (3,2.2) rectangle (5,3.8) node[pos=.5] {RAM};
%
%\end{tikzpicture}
\caption{UMA vs NUMA}
\label{fig:1_HPC:UMA_NUMA}
\end{figure}

\subsubsection{UMA}
The Uniform Memory Access is a global memory shared by every threads or cores. 
In UMA every processors us its own cache as local memory. 
The addresses can be accessed directly by each processors which make the access time ideal. 
The downside is that more processors require more buses and thus UMA is hardly scalable. 
The cache consistancy problem also appears in this context and will be discussed in next part. 
Indeed, if a data is loaded in one processor cache and modifies, this information need to be spread to the memory and maybe other processes cache. 

With the arising of accelerators like GPUs and their own memory, some constructors found ways to create UMA with heterogeneous memory. 
AMD creates the heterogeneous UMA, hUMA~\cite{rogers2013amd}, in 2013 allowing CPU and GPU to target the same memory area.

\subsubsection{NUMA}

In Non Unified Memory Access every processor have access to its own memory but allows other processors to access those area though Lightning Data Transport, LDT or Quick Path Interconnect, QPI, for Intel architectures. 

As we mention for the UMA memory, even if the processors does not directly access to the memory, a cache coherency is important. 
Two methods are possible: on one hand, the most used is Cache-Coherent NUMA (CC-NUMA) were protocols are used to keep data coherency through the memory. on the other hand No Cache NUMA (NC-NUMA) force the processes to avoid cache utilization and write results in main memory losing all the benefits of caching data. 

\subsubsection{COMA}
In Cache-Only Memory Accesses, the whole memory is see as a cache from every processes.
Attraction memory is setting up and will attract the data near the process that will use those data. 
This model is less commonly use and lead to, at best, same results as NUMA.

\subsection{Distributed memory}

The previous models are shared memory, in the case where the processes can access memory of their neighbors processes. 
In some cases, like supercomputer, it would be too heavy for processors to handle the requests of all the others through the network. 
Each process or node will then possess its own local memory, that can be share with local processes. 
Then, in order to access to other nodes memory, communications through the network have to be done and copyed in local memory. 
This distributed memory is called No Remote Memory Access (NoRMA).

\section{FLOPS, Speedup, efficiency and scalability}
In the previous parts we described the differents executions models, characterizations and memory models for HPC. 
Based on those tools we need to be able to emphasis the performances of a computer and a cluster. 

\subsection{FLOPS}
The Floating point Operation Per Second consider the number of floating-point operation that the system will executes in a second. 
They are an unit of performance for computers, higher FLOPS is better. 
This is the scale also use to consider supercomputers computational power. 
For a cluster we can compute the theoretical FLOPS (peak) with:
\begin{equation}
FLOPS_{cluster} = \#nodes \times \frac{\#sockets}{\#nodes} \times \frac{\#cores}{\#sockets} \times \frac{\#GHz}{\#core} \times \frac{FLOPS}{cycle}
\end{equation}

On Fig.\ref{tab:1_HPC:flops_year}, the scale of FLOPS and the year of the first world machine is presented. 

\begin{table}
\[\arraycolsep=1.4pt\def\arraystretch{2.2}
\begin{tabular}{| l | l | l || l | l | l |}
\hline
	%\rowstyle{\bfseries}
	\textbf{Name} & \textbf{FLOPS} & \textbf{Year} & \textbf{Name} & \textbf{FLOPS} & \textbf{Year} \\
	\hline
	\hline
	kiloFLOPS & $10^{3}$ & & petaFLOPS  & $10^{15}$ & 2005 \\ 
	\hline
	megaFLOPS & $10^{6}$ & & exaFLOPS   & $10^{18}$ & 2020 ? \\
	\hline
	gigaFLOPS & $10^{9}$ & $\approx$ 1980  & zettaFLOPS & $10^{21}$ & \\
	\hline
	teraFLOPS & $10^{12}$ & 1996 & yottaFLOPS & $10^{23}$ & \\
	\hline
	\end{tabular}
	\]
	\caption{Floating-point Operation per Second and years in HPC.}
	\label{tab:1_HPC:flops_year}
	\end{table}

	FLOPS is the main way to reprensent a computer's performance but other ways exists like Instructions Per Seconds (IPS) or Operations Per Second (OPS).
	Some benchmarks also provide their own metrics. 

	\subsection{Scalability}
	The scalability express the way a program react to parallelism. 
	When an algorithm is implemented on a serial machine and is ideal to solve a problem, one may consider to use it on more than one core, socket, node or even cluster. 
	Indeed, one may expect less computation time, bigger problem or a combination of both while using more ressources. 
	This completely depend on the algorithm parallelisation and is expressed through scalability. 
	A scalable program will scale on as many processors as we give, whereas a poorly scalable one will give same of even worst results as the serial code.  
	Scalability can be approch using speedup and efficiency.

	\subsection{Speedup and efficiency}
	The latency is the time necessary to complete a task in a program.
	\index{latency} 
	Lower latency is better. 

	The speedup compare the latency of both sequential and parallel algorithm. 
	In order to get relevent results, one may consider the best serial program against the best parallel implementation.

	Considering $n$ the number of processes and $n=1$ the sequential case.
	And $T_n$ the execution time with $n$ processes and $T_1$ with one process, the sequential execution time. 
	The speedup can be defined using the latency by the formula: 
	\index{Speedup}
	\begin{equation}
	\text{speedup} = S_n =  \frac{T_1}{T_n}
	\end{equation}


	\begin{figure}
	\centering 
	\begin{tikzpicture}
	\draw[->] (-.5,0) -- (4,0) node[right] {$n$};
	\draw[->] (0,-.5) -- (0,2.5) node[above] {$speepup$};
	\draw (0,0) -- (4,2) node[right] {\text{Linear Speedup}};
	\draw (0,0) .. controls (3.5,1.3) .. (4.,1.2) node[right] {\text{Typical Speedup}} ;
	\draw (0,0) .. controls (3.5,2.2) .. (4.,2.2) node[above] {\text{Hyperlinear speedup}} ;
	\end{tikzpicture}
	\caption{Observed speedup}
	\label{fig:1_HPC:speedup_obs}
	\end{figure}

	As shown on figure \ref{fig:1_HPC:speedup_obs} several kind of speedup can be observed. 
	\paragraph{Linear}
	The linear speedup usually represents the target for every program in HPC. 
	Indeed, having the speedup growing exactly as the number of processors grows is the ideal case. 
	Codes fall typical into two cases, typical and hyperlinear speedup. 
	\paragraph{Typical speedup}
	This represents the most common observed speedup. 
	As the number of processors grows, the program face several of the HPC walls like communications wall or memory wall. 
	The increasing number of computational power is reduiced to the sequential part or lose time in communications/exchanges. 
	\paragraph{Hyperlinear speedup}
	In some cases we can observe an hyperlinear speedup, meaning that the results in parallel are even better than the ideal case. 
	This can occur if the program can fit exactly in memory for less data on each processors or even fit perfectly for the cache utilization. 
	The parallel algorithm can also be way more efficient than the sequential one. 

	The efficiency is defined by the speedup devided by the number of workers: 
	\index{Efficiency}
	\begin{equation}
	\text{efficiency} = E_n = \frac{S_n}{n} = \frac{T_1}{nT_n}
	\end{equation}
	The efficiency, expressed in percent, represent the evolution of the code stability to growing number of processors. 
	As the number of processes grows, a scalable application will keep an efficiency near 100\%.

	\section{Amdhal's and Gustafson's law}
	The Amdhal's and Gustafson's laws are ways to evaluate the maximal possible speedup for an application taking in account different characteristics. 

	\subsection{Amdahl's law}
	\index{Amdahl's law}
	The Amdahl's law\cite{amdahl1967validity} is use to find the theoretical speedup in latency of a program.
	We can separate a program into two parts, the one that can be execute in parallel and the one that is sequential. 
	The law states that even if we reduce the parallel part using an infinity of processes the sequential part will reach 100\% of the total computation time. 

	Extracted from the Amdahl paper the law can be writen as: 

	\begin{equation}
	S_n = \frac{1}{Seq + \frac{Par}{n}}
	\end{equation}

	Where $Seq + Par = 1$ and $Seq$ and $Par$ respectively the sequential and parallel ratio of a program.
	Here if we use up to $n=\inf$ processes, $S_n \leq \frac{1}{Seq}$ the sequential part of the code become the most time consumming. 

	And the efficiency become:
	\begin{equation}
	E_n = \frac{1}{n\times Seq + Par}
	\end{equation}

	\begin{figure}
	\includegraphics[width=\textwidth]{\locpath/figures/chap1/speedup_laws.png}
	\caption{Theoretical speedup for Amdahl's (left) and Gustafson's (right) law}
	\label{fig:1_HPC:speedup_laws}
	\end{figure}

	A representation of Amdahl's speedup is presented on Fig.~\ref{fig:1_HPC:speedup_laws} with varying percentage of serial part. 
	The parallel part is like $Par = (100-Ser)\%$.

	\subsection{Gustafson's law}
	\index{Gustfson's law}
	The Amdhal's law is focused on time with problem of the same size. 
	John L. Gustafson's idea is that using more computational units, the problem size can grow accordingly. 
	He considered a constant computation time with evolving problem, growing the size accordingly to the number of processes. 
	Indeed the parallel part grows as the problem size do, reducing the percentage of the serial part for the overall resolution.

	The speedup can now be estimated by:
	\begin{equation}
	S_n = Seq + Par \times n
	\end{equation}

	And the effiencity: 
	\begin{equation}
	E_n = \frac{Seq}{n} + Par
	\end{equation}


	Both Amdahl's and Gustafson's law are correct and they represent two solution to check the speedup of our applications. 
	The strong scaling\index{Strong scaling}, looking at how the computation time vary evolving only the number of processes, not the problem size. 
	The weak scaling\index{Weak scaling}, at opposite to strong scaling we look how the computation time evolute varying the problem size keeping the same amount of work per processes. 

	\section{Conclusions}

	In this chapter we presented the different basic tools to be able to understand HPC. 
	The Von Neumann model that represent every nowadays architecture. 
	The Flynn taxonomy that is in constant evolution with new paradigms like recent SIMT from NVIDIA. 
	We also presented the memory types that will be use at different layers in our clusters, from node memory, CPU-GPGPU shared memory space to global fast shared memory. 
	We finished by presenting the most important laws with Amdahl's and Gustafson's laws.
	We introduice the concept of Strong and Weak scaling that will lead our tests through all the examples in Part II and Part III. 
