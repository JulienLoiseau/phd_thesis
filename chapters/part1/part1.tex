\part{HPC and Exascale}
\chapter*{Introduction}

This part is a state of the art of High Performance Computing and describes the tools we need for our study. 
High Performance Computing, HPC, does not have a strict definition. 
The history starts with domain scientists in need of more complex and longer computation for models checking or simulations. 
They developed their own tools beginning  with vacuum tubes computers which can be consider as a milestone in HPC history. 
Since this first machine the technology became more and more complex at every layer. 
The hardware conception, the software to handle it and even the models and topologies. 
HPC is now a field on its own but always dedicated to the end purpose, domain scientist computations. 
HPC experts are interested in supercomputer construction, architecture and code optimization, interconnection behaviors and creating more software, framework or tools to facilitate access to these very complex machines. 

In this part we give a non-exhaustive definition of HPC focusing our needs for this study through three chapters. 

We first focus on what are the theoretic models for the machines and the memory we base our work on. 
This first chapter also present what we define performance for HPC and the main laws that concern it. 

The second chapter details the architecture base on these models. 
We present nowadays dominant constructors and architectures from multi-cores to specific many-cores machines. 
Representative member of today supercomputers are described.  
We show that hybrid architectures seems to be the only plausible way to reach the exascale. 

In the third chapter we detail the main software to target those complex architectures. 
We present tools, frameworks and API for shared and distributed memory. 
We also introduce the mains benchmarks use in HPC world to rank the most powerful supercomputers. 
This chapter also shows that those benchmarks are not the bests to give an accurate score or scale for "realistic" domain scientists applications.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																	%
%	CHAPTER ONE, THEORY of HPC										%
%																	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{\locpath/chapters/part1/chap1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																	%
%	CHAPTER TWO, HARDWARE IN HPV									%
%																	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{\locpath/chapters/part1/chap2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																	%
%	CHAPTER THREE, SOFTWARE AND API									%
%																	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{\locpath/chapters/part1/chap3}

\chapter*{Conclusion}
This part detailed the state of the art theory, hardware and software in High Performance Computing and the tools we need to detail our experiences.

In the first chapter we introduced the models for computation and memory. 
We also detailed the main laws of HPC. 

The second chapter was an overview of hardware architectures in HPC. 
The one that seems to be the most promising regarding computational power and energy consumption seems to be hybrid architectures. 
Supercomputers equipped with classical processors accelerated by devices like GPGPUs, Xeon Phi or, for tomorrow supercomputers, FPGAs. 

In the third section we showed that the tools to target such complex architecture are ready. 
They provide the developer a two or three layer development model with MPI for distribution over processes, OpenMP/PThreads for tasks between the processor's cores and CUDA/OpenCL/OpenMP/OpenACC to target the accelerator. 

We also showed in the last part that the benchmarks proposed to rank those architectures are based on regular computation. 
They are node facing realistic domain scientists code behavior. 
The question that arise is: How the hybrid architecture will handle irregularity in term of computation and communication? 
This question will be developed in the next part through one example for irregular computation and another for irregular communication using accelerators. 



