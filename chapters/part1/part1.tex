\part{HPC and Exascale}
\chapter*{Introduction}

This part of this thesis contains a state of the art presentation for theory and applications of High Performance Computing.
It describes the tools we need for our study. 
High Performance Computing, HPC, does not have a strict definition. 
Its history starts with domain scientists in need of more complex and more reliable computation for models checking or simulations. 
They developed their own tools beginning  with vacuum tubes computers which can be consider as a milestone in HPC history. 
Since this first machine the technology became more and more complex at every layer: the hardware conception, the software to handle it and even the models and topologies.
HPC is now a scientific field on its own but always dedicated to the end purpose, domain scientist computations. 
HPC experts are interested in supercomputer construction, architecture and code optimization, interconnection behaviors and creating more software, framework or tools to facilitate access to these very complex machines. 

In this part we give a non-exhaustive definition of HPC focusing on models, hardware and tools required for our study in three chapters.

We first focus on what are the theoretic models for the machines and the memory we base our work on. 
This first chapter also presents what is defined as performance for HPC and the main laws that define it. 

The second chapter details the architecture base on these models. 
We present nowadays platforms with dominant constructors and architectures from multi-core to specific many-core machines. 
Representative members of today's supercomputers are described.  
We show that hybrid architectures seem to be the only plausible way to reach the exascale: they offer the best performance per watt ratio and nowadays API/tools allow to target them more easily and efficiently than ever.  

In the third chapter we detail the main software to target those complex architectures. 
We present tools, frameworks and API for shared and distributed memory. 
We also introduce the main benchmarks used in the HPC world in order to rank the most powerful supercomputers. 
This chapter also shows that those benchmarks are not the best to give an accurate score or scale for "realistic" domain scientists' applications.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																	%
%	CHAPTER ONE, THEORY of HPC										%
%																	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{\locpath/chapters/part1/chap1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																	%
%	CHAPTER TWO, HARDWARE IN HPV									%
%																	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{\locpath/chapters/part1/chap2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																	%
%	CHAPTER THREE, SOFTWARE AND API									%
%																	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{\locpath/chapters/part1/chap3}

\chapter*{Conclusion}
This part detailed the state of the art theory, hardware and software in High Performance Computing and the tools we need to detail our experiences.

In the first chapter we introduced the models for computation and memory. 
We also detailed the main laws of HPC. 

The second chapter was an overview of hardware architectures in HPC. 
The one that seems to be the most promising regarding computational power and energy consumption seems to be hybrid architectures. 
Supercomputers equipped with classical processors accelerated by devices like GPGPUs, Xeon Phi or, for tomorrow supercomputers, FPGAs. 

In the third section we showed that the tools to target such complex architecture are ready. 
They provide the developer a two or three layer development model with MPI for distribution over processes, OpenMP/PThreads for tasks between the processor's cores and CUDA/OpenCL/OpenMP/OpenACC to target the accelerator. 

We also showed in the last part that the benchmarks proposed to rank those architectures are based on regular computation. 
They are node facing realistic domain scientists code behavior. 
The question that arise is: How the hybrid architecture will handle irregularity in term of computation and communication? 
This question will be developed in the next part through one example for irregular computation and another for irregular communication using accelerators. 



