\chapter*{Introduction}

In the aurora 2020-2021 for United States of America and maybe before, like 2019 for China, the world of High Performance Computing (HPC) will reach another milestone in the power of machines: the Exascale. 
These supercomputers will be $100$ times faster than the estimated overfall operations performed by the human brain and its $10^{16}$ \textbf{FL}oating point \textbf{O}perations \textbf{P}er \textbf{S}econd (FLOPS) and achieve a computational power of a quintillion of ($10^{18}$) FLOPS.
This odyssey started long time ago with the first vacuum tubes computers and the need of ballistic computation for war. 
Nowadays the aim is very nearby and the power of a nation is represented by its army and money but also by the computational power of its supercomputers.
HPC's applications also spread into all the area of science and technology.\\

Since 1962, considering the Cray CDC 6600 as the first supercomputer, the power of those machines have increased following an observation of the co-fonder of the Intel company, Gordon Moore. 
Better known under the name of "Moore's Law", it speculates in 1965 that: considering the constant evolution of technology the number of transistors on a dense integrated circuit will double approximately every two years. 
Thus, the computational power, that intrinsically depends on the number of transistors on the chip, will increase; more important, as "\textit{money is the sinews of war}", the cost of the die for the same performances will decrease.  
This observation can be related to the supercomputers results through the years in the TOP500 list. 
As presented on figure~\ref{fig:intro_top500}, even if estimated in early 1965, the Moore's law seems to be accurate and sustainable. 

\begin{figure}
\includegraphics[width=\linewidth]{figures/introduction/top500_list_approximation.png}
\caption{Computational power evolution in the TOP500 list}
\label{fig:intro_top500}
\end{figure}

This linear evolution is not only driven by the shrink in the semiconductor with smaller transistors.
In fact the first one-core Central Processing Units (CPUs) were made using more transistors but also faster frequency.
They later faced limitations to reach high frequencies, because of the power consumption and the inherent cooling of the heat generated by the chip.
This is why, at some point in early twentieth century, IBM proposed the first multi-core processor on the same die, the Power4.
The constructors started to create chips with more than one core to increase the computational power in conjunction with the shrink of semiconductors, answering the constant demand of more powerful devices and allowing the Moore's law to thrive. 
This increase of the overall power of the chip comes with some downside costs in synchronizations steps between the cores for memory access, work sharing and complexity.
The general purpose CPU nowadays features from 2 to less than a hundred of cores on a single chip.\\

In order to reach even more computational power some researchers started to use many-core approaches. 
Using hundreds of cores these devices are using very "simple" computing units, with slow frequency and low power consumption but add more complexity and requirement for their efficient programming with even more synchronizations needed between the cores. 
Usually those many-core architectures are used coupled with a CPU that sends the data and drives them.
Some accelerators can be driven and drive regarding their configuration like the Intel Xeon Phi. 
Usually called accelerators, those devices are used in addition to the host processor to provide their efficient computational power in the key part of execution. 
The most famous accelerators are the Xeon Phi, the General Purpose Graphics Processing Unit (GPGPU) initially used in graphics, Field Programmable Gates Array (FPGA) or dedicated Application-Specific Integrated Circuit (ASIC).
The model using a host with additional device(s) appears and we will mention it as "Hybrid Architecture".
In fact a cluster can be composed of CPUs, CPUs with accelerator(s) of the same kind, CPUs with heterogeneous accelerators or even accelerators like Xeon Phi driving different kinds of accelerators.\\

Since 2013-2014 many companies, like the Gordon Moore's company \textit{Intel} itself, stated that that the Moore's law is over. 
This can be seen on figure~\ref{fig:intro_top500}: on the right part of the graph, the evolution is not linear anymore and tends to decrease slowly in time. 
This can be imputed to two main factors: on one hand, we slowly reach the maximal shrink size of the transistors implying hard to handle side effects. 
On the other hand the power wall implied by the power consumption required by so many transistors and frequency speed on the chip.

Even with all these devices, nowadays supercomputers are facing several problems in their conception and utilization. 
The three mains walls bounding the overall computational power of the machines are the power consumption wall, the communication wall and the memory wall.  
Some subproblems like the the interconnect wall, resilience wall or even the complexity wall also arise and make the task even harder.\\

In this context of doubts and questions about the future of HPC, this study propose several points of view. 
We think that the future of HPC is made with those hybrid architectures or acceleration devices adapted to the need, using well suited API, Framework and code.
We consider that the classical benchmarks, like the TOP500, are not enough to target the main walls of those architectures and especially accelerators. 
Domain scientists applications like physics/astrophysics/chemist/biologist require benchmarks based on more irregular cases with heavy computation, communications and memory accesses. 

In this document we propose a metric that extracts the three main issues of HPC and apply them to accelerated architectures to figure out how to take advantage of those architectures and what the limitations are. 
The first step to this metrics is obtained when merging two characteristic problems and then a third one combining all our knowledge.
The first two are targeting computation and communication wall over very irregular cases with high memory accesses, using an academic combinatorial problem and the Graph500 benchmark. 
The last is a computational scientific problem that covers both difficulties of the previous problems and appears to be hard to implement on supercomputers and even more on accelerated ones.\\

This thesis is composed of 3 parts.

The first develop the state of the art in HPC from the main laws to the hardware. 	
We go through the basic laws from Amdahl's to Gustafson's laws and the specification of speedup and efficiency.
Classical CPUs, GPGPUs and other accelerators are described and discussed regarding the state of the art. 
The main methods of ranking and the issues regarding them are presented.\\ 

In the second part we propose our metric based on characteristic problems to target classical and hybrid architectures.
The Langford problem is described as an irregular and computationally heavy problem.
This shows how the accelerators, in this case GPUs, are able to support the memory and computation wall. 
This allowed us to beat a world record with the last instances of this academic problem.
The Graph500 problem is then proposed as an irregular and communications heavy problem. 
We present our implementation, and more over the logic to take advantage of the GPUs computational power for realistic applications. \\

In the third part we consider a problem that is heavy and irregular regarding to computation and communications.
We analyze it and show that it combine all the previous limitations. 
Then we apply our methodology and show how nowadays supercomputers can overcome those issues. 
This computational science problem is based on the Smoothed Particle Hydrodynamics method.
Based on our global work to efficiently parallelize those kind of production application, we intend to provide an efficient tool for physicists and astrophysicists called FleCSPH. 
The former application we started with was developed on top of the FleCSI framework from the Los Alamos National Laboratory it allowed us to exchange directly with the LANL domain scientists on their need.\\

The last part will conclude on this work and results and show some of the main prospects of this study and my future researches. 